---
layout:     post
title:      主题模型系列算法详解
subtitle:   NLP 模型之一---LDA模型
date:       2019-03-08
author:     BY
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - 终端
    - zsh
    - Notes
---

要讲主题模型要从概率论中的频率学派和贝叶斯学派讲起。

### 频率学派和贝叶斯学派

首先频率学派的思想很简单，我们观测到的结果就是客观世界的样子，就是直接用实验观测到的数据来描述随机事件，如果试验次数足够多，那么我们观测到的数据就能够准确描述随机实验的分布，比如抛硬币实验，2次向上，8次向下，就会认为这枚硬币正面向上的概率是20%。

而贝叶斯学派的观点是，我们观测的到的不是客观世界的样子，我只是从我的角度出发去描述，从我的角度出发，带有我的先验知识，然后尽量去描述随机事件。比如我先验的认为硬币可能是均匀的，那么正面在上的概率就是50%，假设我观测到2次正面向上，8次反面向上，那么我会用我之前的先验支持去更正现在的概率，那么我可能认为(50+2)/110 = 47.2%，或者其他有另一个比我更了解这枚硬币，那么通过观测结果他认为正面向上的概率是(50000+2)/100010=49.997%。

显然贝叶斯学派的观点更适合我们从机器学习的角度建模解决问题，机器学习就是从拿到的数据中去拟合函数训练模型，而很多时候显然先验知识的加入对解决问题很有帮助。

### Bernoulli分布和Bate分布

先说Bernoulli分布，就是猜测结果为两个的随机事件的概率分布，比如经典的抛硬币，它的概率分布就是一个Bernoulli分布。它的似然函数是

$$
P(data|\theta) \propto \theta^{z}*(1-\theta)^{N-z}
$$

其中$$\theta$$是正面向上的概率值，z是正面向上的次数，N是总次数。

再来说Beta分布，它是概率的概率分布，举个和抛硬币一样经典的例子就是棒球的击球率，均值一般是0.266，我们要建模估计一个运动员的击球率，就要用Beta分布，它表示某个概率值出现的可能性大小。它的似然函数是

$$
Beta(a,b) = \frac{\theta^{a - 1} * (1-\theta)^{b-1}}{B(a,b)} \propto \theta^{a - 1} * (1-\theta)^{b-1}
$$

其中a，b分别表示击中的次数和未击中的次数，或者前面抛硬币正面的次数和反面的次数，而上面贝叶斯学派计算概率就是用的Bate函数的期望计算方法$$u = a/(a+b)$$

前面B函数是一个标准化函数，可以忽略。

前面说了，按照贝叶斯学派的观点，我们现在在已知棒球击球率分布的情况下，通过某个球员的击球实验来估计这个球员的击球率分布。

$$
P(\theta|data) = \frac{P(data|\theta)*P(\theta)}{P(data)} \propto P(data|\theta)*P(\theta)
$$

其中$$P(\theta)$$是先验分布，这里P(data)和我们要估计的已知数据的情况下估计击球率独立，所以可以忽略

其中已知的击球率分布是一个Beta分布，而球员击球的数据是一个Bernoulli分布，我们把Beta分布带入到贝叶斯函数的$$P(\theta)$$中，把Bernoulli分布的似然函数带入到$$P(data \vert \theta)$$,得到

$$
P(\theta|data) \propto \theta^z (1 - \theta)^{N-z} * \theta^{a-1} (1-\theta)^{b-1} \propto \theta^{a+z-1} (1-\theta)^{b+N-z-1}
$$

我们设$$a_{new} = a+z$$，$$b_{new} = b+N-z$$，然后得到一个$$Bate(a_new,b_new)$$分布。

这样一个Beta分布经过贝叶斯公式变换之后还是一个Beta分布，不改变函数本身属性的特性，叫做**共轭**。

### Multinomial分布和Dirichlet分布

和上面的分布一样，把随机结果为两个推广为多个就对应的是Multinomial分布和Dirichlet分布，他们也具有共轭性质。

比如我们常见的掷骰子，结果可能有六个，而且着六个结果概率互斥且概率和为1，发生其中一个结果X次的概率就是Multinomial分布，它的概率似然函数是：

$$
Mult\{\vec{k}|\vec{p},N \} = \frac{N!}{k_1!k_2!...k_n!} \prod_{i=1}^{n} p_i^{k_i}, where \sum_{i=0}^{n} k_i = N
$$

其中n是结果的选项数目，比如骰子的话就是6，N是总实验次数，$$k_i$$表示结果为k的数目，比如点数为3的有8次，相当于我们掷骰子实验，得到每个点数朝上的次数，去估计这个概率分布。

Dirichlet分布类似上面是Multinomial分布的共轭分布，它用来估计Multinomial分布的概率分布，他的概率似然函数为

$$
Dir\{ \vec{p}|\vec{k} \} = \frac{1}{B(k)} \prod_{i=1}^{n} p_i^{k_i-1}, where\ B(k) = \frac{\prod_{i=1}^n\Gamma(k_i)}{\Gamma(\sum_{i=1}^{n}k_i)}
$$

其中n跟上面一样，结果数据，$$p_i$$是某个结果的分布概率，比如骰子6在上面的概率1/6，而$$k_i$$是出现i点在上的次数。

跟二项分布的均值估计方法一样，Dirichlet分布也有类似的结论，他的均值

$$
E(p) = (\frac{k_1}{\sum_{i=1}^{n} k_i}, \frac{k_2}{\sum_{i=1}^{n} k_i},...,\frac{k_n}{\sum_{i=1}^{n} k_i})
$$

### 频率版的Unigram model 

这个是最简单的模型，我们假设通过如下规则生产文本

1. 上帝只有一个骰子，这个骰子有V个面，每个面对应一个词，各个面的概率不一样
2. 每抛一次骰子，抛出的面就对应产生一个词；如果一篇文章中有n个词，上帝独立抛n次骰子产生这n个词。

因为我们假设一篇文章相当于一个袋子，里面装了一些词，这些词的顺序信息被我们忽略了，所以被称作**词袋模型**。

假设总词频是N，其中每个词的$$v_i$$的发生次数是$$k_i$$，各个面的概率记为$$p_i$$。骰子是多面的，所以恰好对应我们前面说的Multinomial分布。

$$
p(\vec{n}) = Mult(\vec{n}|\vec{p},N)
$$

语料的概率为

$$
P(W) = p(\vec{w_1} p(\vec{w_2}) ... p(\vec{w}_m)) = \prod_{k=1}^{V} p_{k}^{n_k}
$$

对于概率的估计，频率学派的估计方法是

$$
p_i = \frac{n_i}{N}
$$

### 贝叶斯版Unigram model 

假设文本是通过下面方式产生的

1. 上帝有一个装着无穷多骰子的坛子，里面有各式各样的骰子，每个骰子有V个面。
2. 上帝从坛子里面抽一个骰子出来，然后用这个骰子不断的抛，产生预料中需要的词。

按照贝叶斯派的思想，坛子里的骰子服从一个先验分布，而骰子确定后通过掷骰子产生文本是一个我们前面说过的Multinomial分布分布，那么他的先验分布我们的最好选择就是他的共轭分布，Dirichlet分布。

在实验已知多项式分布$$Mult(\vec{n} \vert \vec{p}, N)$$的情况下，我们计算喉炎分布

$$
Dir(\vec{p}|\vec{n} + \vec{k}) = Mult(\vec{n} \vert \vec{p}, N) + Dir(\vec{p}|\vec{k})
$$

于是按照前面得到均值

$$
E(\vec{p}) = \frac{n_i + k_i}{\sum_{i=1}^V (n_i + k_i)}
$$

### PLSA模型 

PLSA(Probabilistic Latent Semantic Analysis)假设一篇文章是有多个不同的主题组成，比如讲主题模型的，可能40%谈论概率，20%谈论计算机，20谈论语言学，10%谈论其他，而这些主题又有该主题下对应的词组成，他的生成方式为：

1. 上帝有两种类型的骰子，一类是doc-topic骰子，每个doc-topic骰子有K个面，每个面有一个topic编号；另一类是topic-word骰子，每个骰子有V个面，每个面对应一个词
2. 上帝一共有K个topic-word骰子，每个骰子有一个编号，对应从1到K
3. 每篇文章生成前，上帝都要为这篇文章**制造**一个特定的doc-topic骰子，然后重复如下过程生成文档中的词
   - 投掷这个doc-topic骰子，得到一个topic编号z
   - 选择K个topic-word骰子中编号为z的那个，投掷这个骰子，于是得到了一个词

我们发现按照上面思路生成文章，文章和文章可以交换，一篇文章中的词也可以交换，所以仍然是一个词袋模型，游戏中K个topic-word骰子，我们记为$$\varphi_1,...\varphi_k$$，对于每篇文档$$d_m$$，都会有一个特定的doc-topic骰子$$\theta_m$$,所有对应骰子记为$$\theta_1,...,\theta_m$$,于是第m篇文档$$d_m$$的生成概率为

$$
P(w|d_m) = \sum_{z=1}^{K} p(w|z)p(z|d_m) = \sum_{z=1}^{K} \varphi_{zw} \theta_{mz}
$$

### LDA模型

LDA(Latent Dirichlet Allocation)是PLSA对应的贝叶斯版本，对应的生成过程如下

1. 上帝有两坛骰子，第一坛子对应doc-topic骰子，第二坛子装的topic-word骰子
2. 上帝随机从第二个坛子中独立抽取K个topic-word骰子，编号从1到K
3. 每次生成一篇文档前，上帝先从第一个坛子中随机抽取一个doc-topic骰子，然后重复投掷这个doc-topic骰子，为每个词生成一个tipic编号z，重复上面过程，生成语料中每个词的topic编号
4. 从头到尾，对语料中的每篇文档中的每个topic编号z，选择K个topic-word骰子z中编号为z的那个，投掷这个骰子，生成topic对应的word。

其中包括两个物理过程，第一个是$$\vec{\alpha} -> \vec{\theta_m} -> \vec{z_{m,n}}$$，这个过程表示在生成m篇文档的时候，先从第一个坛子中抽了一个doc-topic骰子$$\vec{\theta_m}$$，然后投掷这个骰子生成了文档中第n个词的topic编号$$z_{m,n}$$，其中第一个过程$$\vec{\alpha} -> \vec{\theta_m}$$显然是一个Dirichlet分布，而第二个过程$$\vec{\theta_m} -> \vec{z_{m,n}}$$对应的Multinomial分布，所以整体是一个Dirichlet-Multinomial共轭结构。

语料中M篇文档的生成过程相互独立，对应M个相互独立的Dirichlet-Multinomial共轭结构，从而整个语料中topic的生成概率为

$$
p(\vec{z}|\vec{\alpha}) = \prod_{m=1}^M p(\vec{z_m}|\vec{\alpha} + \vec{n_m})
$$

对于第二个物理过程$$\vec{\beta} -> \vec{\varphi_k} -> w_{k}$$，跟上面一样，$$\vec{\beta} -> \vec{\varphi_k}$$对应着Dirichlet分布，而$$ \vec{\varphi_k} -> w_{k}$$对应着Multinomial分布，所以整体也是一个Dirichlet-Multinomial共轭结构。

语料中K个topics生成words的过程相互独立，所以对应K个相互独立的Dirichlet-Multinomial共轭结构，从而整个语料中词的生成概率为

$$
p(\vec{w}|\vec{z},\vec{\beta}) = \prod_{k=1}^K p(\vec{w_k}|\vec{z_k}, \vec{\beta}+\vec{n_k})
$$

结合上面俩式得到

$$
p(\vec{w},\vec{z}|\vec{\alpha},\vec{\beta}) = p(\vec{w}|\vec{z},\vec{\beta}) p(\vec{z}|\vec{\alpha}) 
$$

写的不是很好，自己做下记录，后面有更深理解了回来在修改。

---

1. [LDA数学八卦](https://pan.baidu.com/s/1-3lg-hz0E2pbIk2bXDAzjw)
