---
layout:     post
title:      Recommendation bryond Matrix Factorization
subtitle:   矩阵分解的新思路
date:       2018-10-22
author:     yougth
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - Matrix Factorization
    - Recommendation
    - Notes
---


## 摘要

在过去的十年中，矩阵分解已经得到了广泛的研究，并已成为最受欢迎的个性化推荐技术之一。然而，基于矩阵分解的推荐模型中采用的点积不满足不等性，这可能会限制它们的表达性并导致次优解。为了克服这个问题，我们提出了一种称为度量分解的新型推荐技术。我们假设用户和物品可以放置在低维空间中，并且可以使用满足不等式属性的欧几里德距离来测量它们的明确相似度。为了证明其有效性，我们进一步设计了两种度量分解变量，一种用于评分预估，另一种用于个性化项目排序。对许多真实世界数据集进行的大量实验表明，我们的方法在评级预测和项目排名任务方面都大大优于现有的最新技术水平。

## 介绍

从用户评分矩阵中学习低阶潜在结构构成了数十年推荐系统研究的基石[14,19,28,32]。这个被广泛称为矩阵分解[28]的概念已经成为更受欢迎的推荐技术之一。凭借其简洁性和有效性，这一系列技术已经取得了不可思议的成功，并在NetFlix比赛等著名竞赛中获得了成功并取得了广泛的应用。

另一方面，从优势方面来说，基于矩阵分解后的空间向量不仅计算简单，而且效率极高。但是它有个很明显的缺陷就是分解后计算相似度不满足三角不等式，这种计算的传递性对于捕捉用户和物品之间的相似性非常重要。为此，基于欧几里德距离作为相似度的度量学习方法一直被认为更加娴熟和富有表现力。

本文主要关注两种范式的桥接。首次，我们探索了“解开度量向量空间”的概念，即通过因子化学习度量空间中的低秩结构。为此，我们提出的方法，分解度量学习（FML）可以被解释为包括距离而不是评级分数的因子化偏好矩阵。这很容易与标准矩阵因子分解不同，后者通常通过评级得分对用户项目相似性进行编码

我们的方法背后的主要动力是结合两种方法中最好的一个，从而减轻每种分解的固有弱点。更具体地说，MF算法效果受限于简单的点积计算。众所周知，点积不满足三角不等式[25] 2，限制了矩阵分解的表达性，因此导致了次优解。详细的解释可以在[13,15]中找到。此外，实验研究还表明，矩阵分解对于大的潜在向量有过拟合风险，这大大限制了模型的可行性和能力[13,42]。

度量学习的最近推广算法协同度量学习（Collaborative Metric Learning）已经证明能够解决这个问题。然而，它自己本身拥有一系列问题，即向量空间中的过度拥塞[37]。虽然它学习了相似性聚类，但我们假设学习聚类也可能因过度拥塞而受到影响，从而导致次优解。我们在定性实验中进一步证明了这一现象。

为了缓解上述问题，我们提出了一种新颖的技术：分解度量学习（FML）。关键的直觉是首先将偏好转换为距离然后用欧几里德距离替换点积。FML从位置和距离的角度考虑推荐问题，并直接将距离矩阵分解为用户和项目的隐式向量，它不仅可以避免上述缺点，而且还适用于评分预估和物品打分。都是根据估计的空间距离进行的。因此，这可以解释为通过分解来embedding到空间距离。总而言之，这项工作的主要贡献总结如下：

 - 我们提出了一种新颖的技术，即分解度量学习（FML），其中我们探索了在度量向量空间中建立低秩结构的概念。在我们的方法中，用户和物品被表示为多维坐标系统中的点（即，欧氏向量空间）。建议基于用户和物品的相似度，在欧式空间中定义。然而，与其他度量学习方法不同，我们的创新在于把评分矩阵分解到欧式空间中。
 - 我们指定两种FML变体来解决两个经典且完善的推荐任务：评分预估和物品打分。我们的模型可以有效地学习到用户和物品在空间中的位置。
 - 在大规模标准公开数据集上进行的实验表明，我们的模型在评分预估和物品打分方面都大大超过了最先进的模型。这表明我们的简单且效果优秀。

## 背景

### 推荐的矩阵分解

矩阵分解是物品推荐最有效的方法之一。推荐任务的第一版矩阵分解由Simon Funk在Netflix竞赛中进行评分预估。后期研究改进了MF并提出了许多变种。例如，Koren等人。[19]基于用户和项目的偏差以及其他的一些具体特征。Salakhutdinov等。[28,29]将MF解释为概率图形模型，以减轻现实世界数据集中的稀疏性和不平衡性。矩阵分解也可以推广到解决个性化物品排序问题。基于矩阵分解的两个经典top-N推荐模型是：贝叶斯个性化排序（BPR）[26]和加权矩阵分解（WRMF）[16]。BPR从贝叶斯的角度解决了物品排序问题，它的目的是将未观察到的用户-物品对远离观察到的用户-物品对。WRMF是项目排名的另一种有效方法。WRMF使用隐式二进制反馈作为训练实例，并考虑用户物品交互矩阵的空间（包括未观察到的条目）。WRMF用置信度来衡量有效的物品和无效的物品。

### 推荐中的深度模型

如前所述，尽管MF的成功，其功能受到点积的限制。为此，已经进行了多次尝试来克服这个问题。一种方法是将非线性引入矩阵[44,45]。Dziugaite等。[5]通过用非线性神经网络建模用户-物品关系，提出了矩阵分解的神经网络广义化。基本思想是对用户和物品潜在特征的点积应用非线性激活函数。[13]遵循这个方法并提出了神经协作过滤（NeuMF）模型用于个性化排名任务。NeuMF由广义MF和多层感知器组成。它将经典协同过滤问题视为一种分类任务，并通过交叉熵损失优化网络。[42]建议使用去噪自动编码器为交互模型引入非线性。将辅助信息与神经网络结合起来[10-12,24,35,36,39,46,47]。尽管如此，这些研究主要集中在克服点积的局限性，并将边信息建模作为未来的工作。关于这一主题的更全面的资料可以在[44]中找到。

### 推荐中的度量学习

另一个有希望的尝试是直接采用满足三角不等式公理的度量。协同度量学习（CML）[15]就是这样一种方法，它将度量学习推广到协作过程.CML遵循最大边缘最近邻算法（LMNN）[40]的思想。LMNN旨在估计线性变换，以制定最小化预期kNN分类错误的距离度量。LMNN由两个关键操作组成：拉和推。拉操作用于拉动同一类中的实例更近，而推操作用于将不同的标记实例分开。严格地说，CML不学习转换函数，而是学习用户向量和物品向量。它只有一个push项，这意味着CML只能push用户不喜欢的物品，而不提供策略来pull用户喜欢的物品。[15]提到CML的损失函数在遇到噪声数据时会pull出正项。然而，与LMNN相比，这种间接pull力较弱，这可能导致过度拥挤（见图4-右）并push可能的推荐候选者太远，从而导致次优解决方案。

## 建议的方法

在本节中，我们将介绍我们的分解度量学习方法。图1说明了FML的简要概述。假设我们有M个user和N个item，偏好（评分或隐性评分）矩阵表示$$R∈R^{M×N}$$。$$R_{ui}$$的值表示用户u对物品i的偏好,评分矩阵R可以是明确的评分，例如显示评分或隐式反馈。显式可以用于待推荐物品的预估打分，而隐式反馈通常用于个性化排名。这两个任务在推荐区域是常见的，所以不要夸大它们[27]。

![图1](http://127.0.0.1:4000/img/Metric_Factorization/1540258123913.jpg "图1")

简化FML的简化说明。它主要有两个步骤：首先，通过过程1转换偏好矩阵（评级或交互）的距离矩阵。其次，将距离矩阵分解为度量向量空间中的用户和物品的空间位置

### 偏好作为距离

我们的目标是解开度量向量空间，以通过分解来学习用户和物品。矩阵分解通过将偏好矩阵（从显式/隐式反馈）分解为点积来学习用户和潜在因素。偏好矩阵也可以视为相似矩阵。由于相似性和距离是两个相反的概念，我们需要将用户偏好转换为第一个距离。

在这里，我们给出了一个简单而有效的公式，距离计算通过下面公式：

$$
Distance(u,i) = Max Simlarity - Similarity(u,i)   \tag{1}
$$

Max Simlarity是矩阵中的最大评分值，比如上面样例中的5。通过这个转换，我们把喜好程度转换为距离，同时保持了其分布。这个转换可以应用于显式和隐式反馈。距离是空间上可以计算的。并且距离函数满足四个关键条件：非负性，唯一性，对称性和满足三角形不等式[40,43]。在不同的应用领域中使用了许多距离函数，例如离散距离，欧几里德距离，图距离。在欧几里德空间中，两点之间的距离通常通过欧几里德距离（或l2范数距离）来测量。由于其简单的形状和有用的特性，它已广泛应用于机器学习，传感器网络，几何，人脸识别等[3,8,22]。在实践中，通常采用平方欧几里得距离来避免计算方根的麻烦。

假设用户和物品在度量向量空间中的位置通过$$P_u∈R^k$$ 和 $$Q_i∈R^k $$表示。我们使用平方欧几里德距离测量用户和物品之间的距离：

$$
D(u,i) = {||P_u - Q_i||}_2^2   \tag{2}
$$

图1简单地说明了FML的过程。首先，我们通过等式1从偏好矩阵得到距离矩阵，然后我们将距离矩阵分解得到用户和物品的空间位置。之后，如果需要，我们可以轻松通过计算回复偏好矩阵中的每一个值然后去做推荐。

### 因式分解距离矩阵

当只有隐式反馈时，个性化打分是推荐系统中的一项重要任务。在许多现实世界的应用程序中，隐式数据（如购买记录，展示和点击）比显示评分更容易获得，这导致将隐式反馈建模为主要焦点。我们遵循以前的研究，并将隐式反馈定义为二元值，其中正向反馈为1，其他情况为0 [13,18]。

首先，我们使用以下公式将隐式反馈转换为距离

$$
Y_{ui} = a * (1 - R_{ui})   \tag{3}
$$

由于$$R_{ui}$$等于0或1，因此假如$$R_{ui} = 0$$的时候距离$$Y_{ui}=a$$，如果$$R_{ui}=1$$的话距离$$Y_{ui}=0$$，因此控制用户和物品距离非常灵活。

对于ranking任务，考虑到展示而没有点击等这些通常是有益的[13,16]。例如，通过对展示和点击的采样，以成对方式训练贝叶斯个性化排名和协作性网络学习。WRMF和NeuMF采用point wise学习方法，但也考虑负向物品。在这项工作中，采用point wise，因为我们希望直接将距离分解为用户和物品的embeddings。

$$
\mathcal{L}(P,Q) = \sum_{(u,i)\in{R}} c_{ui}(Y_{ui} - \mathcal{D}(u,i))^2    \tag{4}
$$

在这里，所有未观察到的相互作用都被定位为置信度$$c_{ui}$$。它的计算方式为：$$c_{ui} = 1 + \alpha w_{ui}$$，其中$$w_{ui}$$表示隐式反馈。表示隐式反馈的次数，例如，如果用户浏览项目两次，那么$$w_{ui}$$ = 2。对于表达大字数，可以通过log变换[16]。由于此信息通常存在于公开可用的数据集中，因此我们令$$w=R_{ui}$$（0或1）。正则策略将在3.4节中介绍。即使我们采用point wise方案，这样能够使得用户喜欢的物品在空间中离当前用户更近，而且能够是用户不喜欢的在空间距离中更远。与大多数基于度量学习的模型不同，这种模型通过某些强制策略用户特别偏爱的物品距离更远，这种方法中的控制机制为未知物品进入用户区域提供了可能性，这对于推荐任务来说是非常必要的。可以作为一个过滤器从未知待推荐物品中选择物品进行推荐。我们模型的另一个重要特征是它可以间接地聚类有共同喜好物品的用户群。这个属性可以捕捉邻居之间的关系，这也是物品推荐的重要因素[1]。
