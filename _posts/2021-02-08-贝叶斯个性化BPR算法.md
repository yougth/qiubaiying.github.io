---
layout:     post
title:      贝叶斯个性化BPR算法
subtitle:   Bayesian Personalized Ranking Loss算法
date:       2021-02-09
author:     yougth
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - 机器学习
    - Loss
    - 算法
---

优化和损失函数是机器学习的一大分支。其中按照大类分为PointWise，就是通过直接预估单个的物品的得分去做排序，在精排环节中最常用；第二类叫PairWise，就是把排序问题看成是其中物品组成的任意pair，然后对比两两pair之间的顺序，所以样本就是这种物品对，这种在召回环节最常用；第三类是ListWise算法，就是需要考虑待排序的物品中任意之间的顺序，把整个列表当作样本，一般在重排环节用的比较多。当然越后面的算法复杂度是越高，因为从组合的角度来说，组合的情况越往后越多，计算也越复杂。

贝叶斯个性化（BPR）就是其中的比较著名的PairWise算法，最初提出来是用来优化第一代召回算法[矩阵分解](http://yougth.top/2019/02/01/svd%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/)用的，矩阵分解的思路虽然现在用的比较少了，但是基于PairWise的召回loss优化方法被很多现在主流找算法依然使用。

### 算法定义

首先拿到的训练样本就是一个UI矩阵，横轴表示所有的物品，纵轴表示所有的用户，然后矩阵里面填充的结果是用户对当前物品的打分值，可以是显式的隐式的，之前的思路是通过[分解矩阵](http://yougth.top/2019/02/01/svd%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/)来填充里面空的格子，然后去预测用户的偏好。

这里换一个角度看待这个问题，对于每个矩阵里的同一个用户来说，如果物品i的打分比物品j高，也就是说当前用户u相比于物品j更喜欢物品i，我们用(u, i, j)来表示，当然这里有两个假设。

 - 用户u的偏好和其他用户无关
 - 用户u的偏好物品i和j也和其他的物品无关，也就是说他对物品的喜好是相互独立的，不会相互影响

然后基于这样的假设，同样我们要去做矩阵分解，最终分解维度为k的两个矩阵

$$
x_{u,i} = w_u · h_i = \sum_{f=1}^k w_{u,f} · h_{i,f}
$$

### 算法详解

看起来我们要解这样一个问题

$$
P(w_u·h_i | >_u) = P(\theta | >_u) = \frac{P(>_u|\theta)· P(\theta)}{P(>_u)}
$$

公式中$$>_u$$表示的就是对于用户u喜欢物品的偏序关系表示，$$\theta$$为简化后要求的矩阵参数，即最终要求解的结果。

使用贝叶斯公式，转化为上面式子，因为用户偏序关系是固定的输入，所以分母部分客户忽略，我们只看分子部分。

首先看分子中的第一项，使用最大似然估计

$$
\prod_{u \in U} P(>_u | \theta) = \prod_{(u,i,j) \in (UxIxI)} P(i >_u j | \theta)^{\delta(u,i,j) \in D} ·  (1 - P(i >_u j | \theta))^{\delta(u,i,j) \notin D}
$$

其中$$\delta(b)$$是0或者1，当b满足偏序关系时为1，否则为0.

按照这个思路，上式后半部分不满足偏序关系，则次方部分为0，那么整个后半部分为1，而前半部分次方项一定满足，所以次方项为1，化简后得到

$$
\prod_{u \in U} P(>_u | \theta) = \prod_{(u,i,j) \in (UxIxI)} P(i >_u j | \theta)
$$

我们先忽略前面求积部分，对概率部分做变换

$$
P(>_u | \theta) = \sigma(\bar{x}_{u,i,j}(\theta)) = \sigma(\bar{x}_{u,i} - \bar{x}_{u,j}) = sigmoid(\bar{x}_{u,i} - \bar{x}_{u,j}) 
$$

这里假设最终求解出来矩阵中结果为$$\bar{x}$$，首先把里面部分变成了$$x_{u,i} - x_{u,j}$$，设计这个函数的目的是希望当满足偏序关系时结果>0,当不满足偏序关系时结果<0，接着对$$\sigma$$函数使用了sigmoid函数做替代，原因是sigmoid当结果>0的时候趋近于1，<0时趋近于0，正好对前面函数做了约束，而且它性质很好且容易优化。

其实整体看来这个替代还是很漂亮的，把已知参数求解偏序关系的概率转化成这样一个简单的函数形式，这也是这个论文的核心。

先写到这儿，后面有时间在补充这个算法的重点以及它和传统SVD相比较的区别。

