---
layout:     post
title:      机器学习基础系列之一
subtitle:   最大似然估计与交叉熵损失在模型中的应用
date:       2017-06-19
author:     BY
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - 终端
    - zsh
    - Notes
---

### 最大似然估计

最大似然估计(Maximum Likelihood Estimation)，简称MLE，假设现在有个抛硬币机器，我们可以任意使用这个机器抛硬币然后拿到观测结果，假设只有两种情况，正面或者反面，假设抛硬币机器每次出的结果是服从某个已知分布的，而且每次出的结果都不受之前的结果的影响，即独立的，那么最大似然估计就是我们已经拿到观测结果，然后反推具有最大可能（概率最大）导致出现这个分布函数的参数。

是不是挺让人眼前一亮的，这不就是机器学习建模的真是情况嘛，确实是这样的，对于某个待优化场景，我们观测到了所有用户在这个场景的反馈样本，现在需要通过观测的结果去反推样本的分布，设计符合这个分布的模型，最终的目的是找到使得出现当前样本可能性最大的模型，然后去学习他的参数。从这个角度讲，所有的机器学习建模过程都可以叫做最大似然估计的过程，使用这个过程也可以推导出最基础的两个损失函数交叉熵损失和平方差损失。

### 交叉熵损失

做过机器模型应该都用过交叉熵损失吧，为什么他这么常用？来先看看它是个什么

##### 信息量

熵出自信息论，首先一个概念是信息量，信息量的定义公式

$$
I(x) = -log(p(x))
$$ 

假设分布是离散分布，x为可能事件，p(x)为事件发生概率，信息量的函数如下，横轴是概率，竖轴表示信息量。

![log函数](http://yougth.top/img/ml/base_ml_0.png)


> 比如有两个事件：
事件A：巴西队入围了世界杯
事件B：中国队入围了世界杯

那个中包含的信息量多呢，当然是事件B嘛，因为事件B发生的概率太小了，从函数曲线也可以看出来，定义域在[0-1]之间，概率越小越接近0信息量越大，概率越大越接近1，信息量越小，这也符合实际情况。

https://blog.csdn.net/b1055077005/article/details/100152102

### log损失和交叉熵损失区别和联系

### 在youtube论文中的变换


### 是否能用于回归问题
