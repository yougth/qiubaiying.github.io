---
layout:     post
title:      机器学习基础系列之一
subtitle:   最大似然估计与交叉熵损失在模型中的应用
date:       2017-06-19
author:     BY
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - 终端
    - zsh
    - Notes
---

### 最大似然估计

最大似然估计(Maximum Likelihood Estimation)，简称MLE，假设现在有个抛硬币机器，我们可以任意使用这个机器抛硬币然后拿到观测结果，假设只有两种情况，正面或者反面，假设抛硬币机器每次出的结果是服从某个已知分布的，而且每次出的结果都不受之前的结果的影响，即独立的，那么最大似然估计就是我们已经拿到观测结果，然后反推具有最大可能（概率最大）导致出现这个分布函数的参数。

是不是挺让人眼前一亮的，这不就是机器学习建模的真是情况嘛，确实是这样的，对于某个待优化场景，我们观测到了所有用户在这个场景的反馈样本，现在需要通过观测的结果去反推样本的分布，设计符合这个分布的模型，最终的目的是找到使得出现当前样本可能性最大的模型，然后去学习他的参数。从这个角度讲，所有的机器学习建模过程都可以叫做最大似然估计的过程，使用这个过程也可以推导出最基础的两个损失函数交叉熵损失和平方差损失。

### 交叉熵损失

做过机器模型应该都用过交叉熵损失吧，为什么他这么常用？来先看看它是个什么

##### 信息量

熵的概念出自信息论，最基础的是信息量，信息量的定义公式

$$
I(x) = -log(p(x))
$$ 

假设分布是离散分布，x为可能事件，p(x)为事件发生概率，信息量的函数如下，横轴是概率，竖轴表示信息量。

![log函数](http://yougth.top/img/ml/base_ml_0.png)


比如有两个事件：

> 事件A：巴西队入围了世界杯
> 
> 事件B：中国队入围了世界杯

那个中包含的信息量多呢，当然是事件B嘛，因为事件B发生的概率太小了，从函数曲线也可以看出来，定义域在[0-1]之间，概率越小越接近0信息量越大，概率越大越接近1，信息量越小，这也符合实际情况。

##### 熵

熵表示信息量的期望，公式为

$$
H(x) = - \sum_{i=1}^n p(x_i)·log(p(x_i))
$$

还是上面的例子，比如事件A发生概率0.9，事件B发生概率0.1，则信息熵为

$$
H(x) = 0.9 * -log(0.9) + 0.1 * -log(0.1) = 0.095 + 0.230  = 0.325 
$$

##### 相对熵

假设有两个相互独立的概率分布P(x)和Q(x)，一般用KL散度(Kullback-Leibler (KL) divergence)来定义这两个分布的差异，KL散度的计算公式为：

$$
D_{KL}(p \| q) = \sum_{i=1}^n p(x_i)·log(\frac{p(x_i)}{q(x_i)})
$$

其中n表示所有事件的个数

##### 交叉熵

对上面公式做变形

$$
D_{KL}(p \| q) = \sum_{i=1}^n p(x_i)·log(p(x_i)) - \sum_{i=1}^n p(x_i)·log(q(x_i))
$$

$$
 D_{KL}(p \| q) = -H(p(x)) + [-\sum_{i=1}^n p(x_i)·log(q(x_i))]
$$

可以看到前一部分就是概率分布P的熵，而后一部分就是交叉熵

$$
H(p,q) = -\sum_{i=1}^n p(x_i)·log(q(x_i))
$$

对应到机器学习模型中，P表示观测到的实际分布，就是我们所谓的label，而Q模型预测出来的分布，就是所谓的predict score。其实本质是用KL散度来衡量预测分布和真实分布的差异，但是看前面公式会发现，化简后第一部分是分布P的熵，而P是实际分布是个固定的值，所以这部分可以忽略，也就是在机器学习问题中交叉熵等于KL散度，这也是为什么分类问题最常用用交叉熵做损失函数。

### log损失和交叉熵损失区别和联系

还记得学习[逻辑回归](http://yougth.top/2017/09/11/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/)的时候，逻辑回归使用最大似然估计得到的log损失，或者叫logistic损失。

$$
\begin{eqnarray*} L_log & = \prod_{i=1}^n P(y_i = 1 \vert x_i) · P(y_i = 0 \vert x_i) \\
& = \prod_{i=1}^n P(y_i = 1 \vert x_i)^{y_i} · P(y_i = 0 \vert x_i)^{1 - y_i}
\end{eqnarray*}
$$

### 在youtube论文中的变换


### 交叉熵能否用于回归问题

---
https://blog.csdn.net/b1055077005/article/details/100152102
