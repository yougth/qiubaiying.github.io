---
layout:     post
title:      线性回归多重共线性优化
subtitle:   线性回归终极优化
date:       2017-09-12
author:     yougth
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - 机器学习
    - 过拟合
    - 预测
    - 算法
    - 多重共线性
    - 岭回归
    - LASSO回归
    - Glmnet
---

## 问题引入

之前分析了[**线性回归**](http://blog.csdn.net/y990041769/article/details/69567838)问题的解法，最小二乘回归法，但是对于大多数的实际问题，由于我们要用有限的观测值去估计模型的分布，比如在之前讲线性回归中的例子，给出的样例有100对，而我们建立的模型是一条直线，我们都知道两点确定一条直线，这里有100个点，这种称作过度确定估计，同时很多样例由于各种原因本身存在误差，另一个方面是特征之间相关性很大，说白了就是两个特征之间存在关系，本身可以用一个变量来表示，这样既简化了模型，同时减少特征意味着减小误差，我们现在在线性回归中去想办法优化这个问题。
> **多重共线性**：是指多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。比如虚拟变量陷阱即有可能触发多重共线性问题。

如果样本存在很大误差，那么我们估计到的结果$\beta$变化就会非常大，估计到的参数的方差也会很大，导致估计不准确。


-----------
## LASSO回归

LASSO回归的思路是既然会导致 $$\beta$$ 变化很大, 且 $$E = mc^2$$，以及方差很大，那么我们在最小二乘估计的时候把 $\beta$ 也作为损失函数中优化的一项，然后让 $\beta$ 的值不能过大

同时在这一项中给一个系数k，就能够调节它的影响，k值越大，则 $\beta$ 的变化影响很大，然后误差导致共线性的影响减小，我们在不断增大惩罚系数的过程中，画出参数 $\beta$ 的变化曲线，这个曲线被称为岭迹
然后如果某个特征参数导致岭迹波动很大，说明这个变量参数有共线性，我们可以考虑剔除这个变量

步骤
 1. 对数据做标准化，从而方便以后对 $\beta_k$ 的岭迹的比较，否则不同变量的参数大小没有比较性。
 2.构建惩罚函数，对不同的k，画出轨迹图
 3. 根据轨迹图，选择剔除掉哪些变量。 

---------

目标函数
![这里写图片描述](http://img.blog.csdn.net/20170912174313366?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveTk5MDA0MTc2OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

我们通过不断增大惩罚系数k，然后看看它的参数各个 $\beta$ 的变化
![这里写图片描述](http://img.blog.csdn.net/20170912174556158?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveTk5MDA0MTc2OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

上面右图是LASSO回归图像，可以看到从右到左随着k值的增大，很多 $\beta$ 会出现变为0的情况，这种变量就是导致共线性的变量，我们可以剔除

为什么出现0的就是导致共线性的变量呢？
![这里写图片描述](http://img.blog.csdn.net/20170912174903153?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveTk5MDA0MTc2OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

看看上面这个图，假设现在是一个两个参数的直线模型，有两个参数 $\beta_1$  $\beta_2$ 需要估计，而按照LASSO对 $\beta$ 的约束条件就是上面蓝色正方形， ${\beta}^{\^}$ 使我们的自变量系数，需要估计的结果，可以看到红色的圆形总是先喝举行的角相交，而这个点正好$\beta_1 = 0$ ，说明传统的最小二乘回归中$\beta_1$  是具有共线性的，我们可以剔除，LASSO回归得到的是只有 $\beta_2$ 对模型有用。

其实除了常用LASSO回归之外，还有一个岭回归，差别在于约束条件是$\sum_{j=1}^p {\beta_j}^2<=t$,对应到上面的图中就是蓝色区域是个圆形，所以他不能直接让$\beta$ 值为0，应为相交的区域肯定不是在坐标轴上 ，但是它也能通过增大惩罚系数k观察$\beta$ 轨迹的波动去剔除共线性变量， 但是很明显发现LASSO更简单。

## 总结
岭回归（Ridge回归）能够优化点是通过调整惩罚系数$ 0< k < 1 $的大小，然后抑制过数据误差导致的拟合
而LASSO回归通过调整惩罚系数观察结果$ \beta $ 先变为0，然后剔除共线性特征，从而达到简化模型作用
目前最好拟合广义线性模型的是glmnet，是LASSO回归的发明人开发的，它通过一系列不同的惩罚系数 $k$ 值去拟合模型，每次拟合都用上一次拟合的结果，从而达到抑制过拟合以及选择变量的效果，有兴趣可以研究下，有开源的库。
当然这些优化都可以用在[逻辑回归](http://blog.csdn.net/y990041769/article/details/70186485)上，逻辑回归只是在线性回归的模型上套了一个logit函数，数据本身误差和共线性的问题同样存在。
